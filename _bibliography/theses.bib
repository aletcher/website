---
---

#@string{aps = {American Physical Society,}}

@thesis{msc,
  abbr={MSc},
  title={Stability and Exploitation in Differentiable Games},
  author={A. Letcher},
  pdf={msc_thesis.pdf},
  abstract={While machine learning has traditionally focused on optimising single loss functions, a growing number of algorithms deal with multiple interacting goals, from GANs to multi-agent RL. Naively transposing gradient descent has been shown to fail, while state-of-the-art proposals are tailored to specific applications (e.g. two-player zero-sum games) or lack strong theoretical guarantees. We provide a unified approach to this problem and prove local convergence of Consensus Optimisation, Symplectic Gradient Adjustment and lookahead in all differentiable games. Learning with Opponent-Learning Awareness (LOLA) takes a different approach by shaping and exploiting opponent learning, reaching better equilibria and outperforming previous methods. On the flip side, we construct the first example where this backlashes in self-play, producing arrogant behaviour and poor losses. We address and solve this catch-22 with a new algorithm named Stable Opponent Shaping (SOS), inheriting strong convergence guarantees from lookahead and shaping capacity from LOLA. We obtain convergence of LOLA to equilibria in two-player zero-sum and n-player cooperative games as a corollary.},
  year={2018}
}

@thesis{bsc,
  abbr={BSc},
  title={Algebraic Features of Multiple Zeta Values},
  author={A. Letcher},
  pdf={bsc_thesis.pdf},
  abstract={While machine learning has traditionally focused on optimising single loss functions, a growing number of algorithms deal with multiple interacting goals, from GANs to multi-agent RL. Naively transposing gradient descent has been shown to fail, while state-of-the-art proposals are tailored to specific applications (e.g. two-player zero-sum games) or lack strong theoretical guarantees. We provide a unified approach to this problem and prove local convergence of Consensus Optimisation, Symplectic Gradient Adjustment and lookahead in all differentiable games. Learning with Opponent-Learning Awareness (LOLA) takes a different approach by shaping and exploiting opponent learning, reaching better equilibria and outperforming previous methods. On the flip side, we construct the first example where this backlashes in self-play, producing arrogant behaviour and poor losses. We address and solve this catch-22 with a new algorithm named Stable Opponent Shaping (SOS), inheriting strong convergence guarantees from lookahead and shaping capacity from LOLA. We obtain convergence of LOLA to equilibria in two-player zero-sum and n-player cooperative games as a corollary.},
  year={2017}
}
