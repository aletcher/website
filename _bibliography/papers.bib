---
---

#@string{aps = {American Physical Society,}}

@article{global,
  title={On the Impossibility of Global Convergence in Multi-Loss Optimization},
  author={A. Letcher},
  journal={International Conference on Learning Representations},
  year={2021},
  abbr={ICLR},
  code={https://github.com/aletcher/impossibility-global-convergence},
  html={https://openreview.net/pdf?id=NQbnPjPYaG6},
  abstract={Under mild regularity conditions, gradient-based methods converge globally to a critical point in the single-loss setting. This is known to break down for vanilla gradient descent when moving to multi-loss optimization, but can we hope to build some algorithm with global guarantees? We negatively resolve this open problem by proving that any reasonable algorithm will exhibit limit cycles or diverge to infinite losses in some differentiable game, even in two-player games with zero-sum interactions. A reasonable algorithm is simply one which avoids strict maxima, an exceedingly weak assumption since converging to maxima would be the opposite of minimization. This impossibility theorem holds even if we impose existence of a strict minimum and no other critical points. The proof is constructive, enabling us to display explicit limit cycles for existing gradient-based methods. Nonetheless, it remains an open question whether cycles arise in high-dimensional games of interest to ML practitioners, such as GANs or multi-agent RL.},
  arxiv={2005.12649}
}

@article{ridge,
  title={Ridge Rider: Finding Diverse Solutions by Following Eigenvectors of the Hessian},
  author={J. Parker-Holder and L. Metz and C. Resnick and H. Hu and A. Lerer and A. Letcher and A. Peysakhovich and A. Pacchiano and J. Foerster},
  abbr={NIPS},
  journal={Advances in Neural Information Processing Systems},
  year={2020},
  abstract={Over the last decade, a single algorithm has changed many facets of our lives - Stochastic Gradient Descent (SGD). In the era of ever decreasing loss functions, SGD and its various offspring have become the go-to optimization tool in machine learning and are a key component of the success of deep neural networks (DNNs). While SGD is guaranteed to converge to a local optimum (under loose assumptions), in some cases it may matter which local optimum is found, and this is often context-dependent. Examples frequently arise in machine learning, from shape-versus-texture-features to ensemble methods and zero-shot coordination. In these settings, there are desired solutions which SGD on 'standard' loss functions will not find, since it instead converges to the 'easy' solutions. In this paper, we present a different approach. Rather than following the gradient, which corresponds to a locally greedy direction, we instead follow the eigenvectors of the Hessian, which we call "ridges". By iteratively following and branching amongst the ridges, we effectively span the loss surface to find qualitatively different solutions. We show both theoretically and experimentally that our method, called Ridge Rider (RR), offers a promising direction for a variety of challenging problems.},
  html={https://papers.nips.cc/paper/2020/hash/08425b881bcde94a383cd258cea331be-Abstract.html},
  arxiv={2011.06505},
  blog={https://bair.berkeley.edu/blog/2020/11/13/ridge-rider/}
}

@article{game,
  title={Differentiable Game Mechanics},
  author={A. Letcher and D. Balduzzi and S. Racani{\`e}re and J. Martens and J. Foerster and K. Tuyls and T. Graepel},
  abbr={JMLR},
  journal={Journal of Machine Learning Research},
  year={2019},
  abstract={Deep learning is built on the foundational guarantee that gradient descent on an objective function converges to local minima. Unfortunately, this guarantee fails in settings, such as generative adversarial nets, that exhibit multiple interacting losses. The behavior of gradient-based methods in games is not well understood -- and is becoming increasingly important as adversarial and multi-objective architectures proliferate. In this paper, we develop new tools to understand and control the dynamics in n-player differentiable games. The key result is to decompose the game Jacobian into two components. The first, symmetric component, is related to potential games, which reduce to gradient descent on an implicit function. The second, antisymmetric component, relates to Hamiltonian games, a new class of games that obey a conservation law akin to conservation laws in classical mechanical systems. The decomposition motivates Symplectic Gradient Adjustment (SGA), a new algorithm for finding stable fixed points in differentiable games. Basic experiments show SGA is competitive with recently proposed algorithms for finding stable fixed points in GANs -- while at the same time being applicable to, and having guarantees in, much more general cases.},
  html={http://www.jmlr.org/papers/v20/19-008.html},
  code={https://github.com/deepmind/symplectic-gradient-adjustment},
  arxiv={1905.04926}
}

@article{sos,
  title={Stable Opponent Shaping in Differentiable Games},
  author={A. Letcher and J. Foerster and D. Balduzzi and T. Rockt{\"a}schel and S. Whiteson},
  journal={International Conference on Learning Representations},
  arxiv={1811.08469},
  abbr={ICLR},
  html={https://openreview.net/forum?id=SyGjjsC5tQ},
  code={https://github.com/aletcher/stable-opponent-shaping},
  abstract={A growing number of learning methods are actually games which optimise multiple, interdependent objectives in parallel -- from GANs and intrinsic curiosity to multi-agent RL. Opponent shaping is a powerful approach to improve learning dynamics in such games, accounting for the fact that the 'environment' includes agents adapting to one another's updates. Learning with Opponent-Learning Awareness (LOLA) is a recent algorithm which exploits this dynamic response and encourages cooperation in settings like the Iterated Prisoner's Dilemma. Although experimentally successful, we show that LOLA can exhibit 'arrogant' behaviour directly at odds with convergence. In fact, remarkably few algorithms have theoretical guarantees applying across all differentiable games. In this paper we present Stable Opponent Shaping (SOS), a new method that interpolates between LOLA and a stable variant named LookAhead. We prove that LookAhead locally converges and avoids strict saddles in all differentiable games, the strongest results in the field so far. SOS inherits these desirable guarantees, while also shaping the learning of opponents and consistently either matching or outperforming LOLA experimentally.},
  year={2019}
}

@article{bodyworn,
  author={A. Letcher and J. Trišović and C. Cademartori and X. Chen and J. Xu},
  journal={IEEE International Conference on Acoustics, Speech and Signal Processing},
  title={Automatic Conflict Detection in Police Body-Worn Audio},
  year={2018},
  abbr={ICASSP},
  arxiv={1711.05355},
  html={https://ieeexplore.ieee.org/document/8461425},
  abstract={Automatic conflict detection has grown in relevance with the advent of body-worn technology, but existing metrics such as turn-taking and overlap are poor indicators of conflict in police-public interactions. Moreover, standard techniques to compute them fall short when applied to such diversified and noisy contexts. We develop a pipeline catered to this task combining adaptive noise removal, non-speech filtering and new measures of conflict based on the repetition and intensity of phrases in speech. We demonstrate the effectiveness of our approach on body-worn audio data collected by the Los Angeles Police Department.}
}
